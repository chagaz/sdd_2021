%-*- coding: iso-latin-1 -*-

\section{QCM}
\paragraph{Question 1.} Soit $(x_1, x_2, \dots, x_n)$ un échantillon d'une
variable aléatoire $X$. On suppose que $X$ suit une loi paramétrisée
par $\gamma$. La vraisemblance de $(x_1, x_2, \dots, x_n)$ est donnée par
\begin{itemize}
\item[$\square$] $\PP(x_1, x_2, \dots, x_n, \gamma)$
\item[$\square$] $\PP(x_1, x_2, \dots, x_n | \gamma)$
\item[$\square$] $\PP(\gamma | x_1, x_2, \dots, x_n)$
\item[$\square$] $\prod_{i=1}^n \PP(x_i|\gamma)$
\item[$\square$] $\prod_{i=1}^n \PP(\gamma|x_i)$
\end{itemize}

\paragraph{Question 2.} Soit $X$ une loi exponentielle de paramètre
$\lambda$. L'estimateur par maximum de vraisemblance de $\lambda$ est donné par
\begin{itemize}
\item[$\square$] $L_n = n \ln(\lambda) - \lambda \sum_{i=1}^n X_i,$ où $(X_1, X_2, \dots, X_n)$ est un échantillon aléatoire de $X$
\item[$\square$] $\widehat{\lambda} = n \ln(\lambda) - \lambda \sum_{i=1}^n x_i,$ où $(x_1, x_2, \dots, x_n)$ est un échantillon aléatoire de $X$
\item[$\square$] $L_n = \frac{n}{\sum_{i=1}^n X_i},$ où $(X_1, X_2, \dots, X_n)$ est un échantillon aléatoire de $X$
\item[$\square$] $\widehat{\lambda} = \frac{n}{\sum_{i=1}^n x_i},$ où $(x_1, x_2, \dots, x_n)$ est un échantillon aléatoire de $X$.
\end{itemize}

\paragraph{Question 3. $\bigstar$} L'estimateur de Bayes est plus proche de l'espérance a
priori que de l'estimateur par maximum de vraisemblance quand la taille de
l'échantillon est
\begin{itemize}
\item[$\square$] grande
\item[$\square$] petite
\item[$\square$] ça dépend.
\end{itemize}



\section*{Solution}

{%
\noindent
\rotatebox[origin=c]{180}{%
\noindent
\begin{minipage}[t]{\linewidth}
\paragraph{Question 1.} Par définition (cf. équation~(3.7) du poly),
\[
L(x_1, x_2, \dots, x_n; \gamma) = \PP(x_1, x_2, \dots, x_n | \gamma) = \prod_{i=1}^n \PP(x_i|\gamma).
\]

\paragraph{Question 2.} 
Par définition la vraisemblance d'un échantillon $(x_1, x_2, \dots, x_n)$ est donnée par
\[
  L(x_1, x_2, \dots, x_n; \lambda) = \prod_{i=1}^n \lambda e^{- \lambda x_i}
  = \lambda^n \prod_{i=1}^n e^{- \lambda x_i},
\] 
et donc sa \textit{log-vraisemblance} vaut 
\[
  \ell(x_1, x_2, \dots, x_n; \lambda) = \ln \left(\lambda^n \prod_{i=1}^n e^{- \lambda x_i}\right)
  = n \ln(\lambda) - \lambda \sum_{i=1}^n x_i.
\]
La fonction $\lambda \mapsto n \ln(\lambda) - \lambda \sum_{i=1}^n x_i$ est
concave sur $]0, +\infty[ \rightarrow \RR$ et on peut donc la maximiser en
annulant sa dérivée.

On obtient \textit{l'estimation par maximum de vraisemblance} de $\lambda$ suivante :
\[
  \hatmle{\lambda} = \frac{n}{\sum_{i=1}^n x_i}
\]
et, si on appelle $(X_1, X_2, \dots, X_n)$ un échantillon aléatoire de $X$, on
obtient \textit{l'estimateur par maximum de vraisemblance} de $\lambda$ :
\[
  L_n = \frac{n}{\sum_{i=1}^n X_n}.
\]


\paragraph{Question 3.} La tendance que nous avons observée sur l'exemple de la
section 3.6 (cf. « Remarque importante ») se vérifie en général : plus on
observe d'échantillons, plus on s'éloigne de l'a priori pour se rapprocher d'un
estimateur issu uniquement des données.
\end{minipage}%
}%
