{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PC 2 : Réduction de dimension - 10 juin 2020 - Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce notebook, nous allons effectuer une analyse en composantes principales d'un jeu de données décrivant les scores obtenus par les meilleurs athlètes ayant participé en 2004 à une épreuve de décathlon, aux Jeux Olympiques d'Athènes ou au Décastar de Talence.\n",
    "\n",
    "Nous utiliserons pour cela la toolbox [`scikit-learn`](http://scikit-learn.org/stable/index.html). `scikit-learn` est un ensemble de librairies de machine learning très utilisée et qui implémente la plupart des algorithmes d'apprentissage automatique non-profond. Il s'agit d'un projet Open Source. \n",
    "\n",
    "Dans une deuxième partie facultative, vous pourrez vous essayer à implémenter l'ACP vous-mêmes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librairies python \n",
    "\n",
    "Ce notebook a été créé avec les versions suivantes des librairies :\n",
    "* numpy 1.20.2\n",
    "* matplotlib 3.4.2\n",
    "* pandas 1.2.4\n",
    "* sklearn 0.24.2\n",
    "* seaborn 0.11.1\n",
    "\n",
    "Tout devrait bien se passer si vous êtes bien dans l'environnement conda `sdd2021` chargé grâce au fichier `environment.yml` à la racine du repo. Des différences de version peuvent expliquer des comportements inattendus (avertissements, messages d'erreurs, fonctionalités inexistantes) mais il n'est pas nécessaire a priori d'avoir exactement les versions listées ci-dessus. Si vous souhaitez vérifier les versions des différentes librairies que vous utilisez, vous pouvez utiliser (après les import correspondant bien sûr) le code suivant :\n",
    "```python\n",
    "print(np.__version__)\n",
    "print(matplotlib.__version__)\n",
    "print(pd.__version__)\n",
    "print(sklearn.__version__)\n",
    "print(sns.__version__)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# charger numpy as np, matplotlib as plt\n",
    "%pylab inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('font', **{'size': 14}) # règle la taille de police globalement pour les plots (en pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données sont contenues dans le fichier `decathlon.txt`.\n",
    "\n",
    "Le fichier contient 42 lignes et 13 colonnes.\n",
    "\n",
    "La première ligne est un en-tête qui décrit les contenus des colonnes.\n",
    "\n",
    "Les lignes suivantes décrivent les 41 athlètes.\n",
    "\n",
    "Les 10 premières colonnes contiennent les scores obtenus aux différentes épreuves.\n",
    "La 11ème colonne contient le classement.\n",
    "La 12ème colonne contient le nombre de points obtenus.\n",
    "La 13ème colonne contient une variable qualitative qui précise l'épreuve (JO ou Décastar) concernée.\n",
    "\n",
    "Nous allons examiner ces données avec la librairie `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "my_data = pd.read_csv('data/decathlon.txt', sep=\"\\t\")  # lire les données dans un dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Quelques rappels sur `pandas`\n",
    "\n",
    "Assurez-vous que vous comprenez l'usage des fonctionalités ci-dessous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Lister les noms des colonnes\n",
    "my_columns = list(my_data.columns) \n",
    "print(my_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionner une colonne\n",
    "my_data['100m'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionner plusieurs colonnes\n",
    "columns = ['100m', '400m']\n",
    "my_data[columns].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionner des lignes à partir de la valeur d'une colonne\n",
    "my_data[my_data['Competition']=='OlympicG'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Dimensions des données\n",
    "print(my_data.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valeurs du tableau sous forme d'array numpy\n",
    "my_data_values = my_data.values\n",
    "print(type(my_data_values))\n",
    "print(my_data_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Type des colonnes\n",
    "print(my_data.dtypes)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploration des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Couples de variables\n",
    "Représentons les différents athlètes selon leur performance au 400m et au lancer du poids (`Shot.put`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(my_data['400m'], my_data['Shot.put'], s=50)\n",
    "\n",
    "plt.xlabel('Performance au 400m')\n",
    "plt.ylabel('Performance au lancer du poids')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les fourchettes de valeur prises par ces deux variables sont différentes, leurs unités aussi. Pour mieux les comparer il est souhaitable de les standardiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5)) # forcer une figure carrée\n",
    "\n",
    "data_400m_std = (my_data['400m'] - np.mean(my_data['400m']))/(np.std(my_data['400m']))\n",
    "data_shotput_std = (my_data['Shot.put'] - np.mean(my_data['Shot.put']))/(np.std(my_data['Shot.put']))\n",
    "\n",
    "\n",
    "plt.scatter(data_400m_std, data_shotput_std, s=50)\n",
    "\n",
    "plt.xlabel('400m (performance standardisée)')\n",
    "plt.ylabel('Lancer du poids (performance standardisée)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question :__ Ce nuage de points vous semble-t-il suggérer que les deux performances sont corrélées ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Réponse :__ Non (cf. les exemples à la fin du Chapitre 2 du poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons calculer leur coefficient de corrélation de Pearson grâce au module `scipy.stats`. La fonction `pearsonr` renvoie même une p-valeur : https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, pval = st.pearsonr(my_data['400m'], my_data['Shot.put'])\n",
    "print(\"Corrélation entre le lancer du poids et le 400m : %.2f (p=%.2e)\" % (r, pval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question :__ Cela vous semble-t-il en accord avec le nuage de points ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Réponse :__ Oui. L'absence de corrélation est confirmée par la p-valeur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question :__ Choisissez deux variables dont il vous semble vraisemblable qu'elles soient corrélées. Affichez le nuage de points correspondant. Calculez le coefficient de corrélation de Pearson. Souhaitez-vous rejeter votre hypothèse ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Réponse (exemple) :__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, pval = st.pearsonr(my_data['100m'], my_data['110m.hurdle'])\n",
    "print(\"Corrélation entre le 100m et le 110m haies : %.2f (p=%.2e)\" % (r, pval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5)) # forcer une figure carrée\n",
    "\n",
    "data_100m_std = (my_data['100m'] - np.mean(my_data['100m']))/(np.std(my_data['100m']))\n",
    "data_hurdles_std = (my_data['110m.hurdle'] - np.mean(my_data['110m.hurdle']))/(np.std(my_data['110m.hurdle']))\n",
    "\n",
    "\n",
    "plt.scatter(data_100m_std, data_hurdles_std, s=50)\n",
    "\n",
    "plt.xlabel('100m (performance standardisée)')\n",
    "plt.ylabel('110m haies (performance standardisée)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sans surprise, les performances au 110m haies et au 100 m sont corrélées."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 [Pour aller plus loin] Matrice de nuages de points avec pandas\n",
    "Une matrice de nuages de points est une visualisation en k x k panneaux des relations deux à deux entre k variables :\n",
    "* sur la diagonale, l'histogramme pour chacune des variables \n",
    "* hors diagonale, les nuages de points entre deux variables (non standardisées).\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.plotting.scatter_matrix.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "scatter_matrix(my_data[['Shot.put','High.jump', '400m']], alpha=0.5, s=60,\n",
    "               figsize=(8, 8));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 [Pour aller plus loin] Seaborn\n",
    "La librairie `seaborn` permet des visualisations plus élaborées que `matplotlib`. Vous pouvez par exemple explorer les capacités de `jointplot`. \n",
    "https://seaborn.pydata.org/generated/seaborn.jointplot.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# fancier plot with seaborn : https://seaborn.pydata.org/\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "sns.jointplot(x='Shot.put', y='400m', data = my_data, \n",
    "              height=6, space=0, color='b')\n",
    "\n",
    "sns.jointplot(x='Shot.put', y='400m', data = my_data, \n",
    "              kind='reg', height=6, space=0, color='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ACP des scores aux 10 épreuves\n",
    "\n",
    "Nous allons maintenant effectuer une analyse en composantes principales des scores aux 10 épreuves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Création de l'array numpy correspondant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(my_data.drop(columns=['Points', 'Rank', 'Competition']))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question :__ \n",
    "* Quel est le nombre d'individus dans la matrice de données `X` ?\n",
    "* Quel est le nombre de variables dans la matrice de données `X` ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Réponse :__ 41 individus, 10 variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Standardisation des données\n",
    "\n",
    "Plus haut, nous avons standardisé les variables nous-mêmes en leur retirant leur moyenne et en les divisant par leur écart-type. Cette tâche est entièrement automatisée par `scikit-learn` :\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scale = preprocessing.StandardScaler().fit(X)\n",
    "X_scaled = std_scale.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Calcul des composantes principales\n",
    "\n",
    "Les algorithmes de factorisation de matrice de `scikit-learn` sont inclus dans le module `decomposition`. Pour  l'ACP, référez-vous à : \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarque : Nous avons ici peu de variables et pouvons nous permettre de calculer toutes les PC. \n",
    "\n",
    "La plupart des algorithmes implémentés dans `scikit-learn` suivent le fonctionnement suivant : \n",
    "* on instancie un objet, correspondant à un type d'algorithme/modèle, avec ses hyperparamètres (ici le nombre de composantes)\n",
    "* on utilise la méthode `fit` pour passer les données à cet algorithme\n",
    "* les paramètres appris sont maintenant accessibles comme arguments de cet objet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciation d'un objet PCA pour 10 composantes principales\n",
    "pca = decomposition.PCA(n_components=10)\n",
    "print(type(pca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On passe maintenant les données standardisées à cet objet\n",
    "# C'est ici que se font les calculs\n",
    "pca.fit(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Proportion de variance expliquée par les PCs\n",
    "\n",
    "Nous allons maintenant afficher la proportion de variance expliquée par les différentes composantes. Il est accessible dans le paramètre `explained_variance_ration_` de notre objet `pca`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1, 11), pca.explained_variance_ratio_, marker='o')\n",
    "\n",
    "plt.xlabel(\"Nombre de composantes principales\")\n",
    "plt.ylabel(\"Proportion de variance expliquée\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question :__ Affichez la proportion *cumulative* de variance expliquée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Réponse :__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cumsum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1, 11), np.cumsum(pca.explained_variance_ratio_), marker='o')\n",
    "\n",
    "plt.xlabel(\"Nombre de composantes principales\")\n",
    "plt.title(\"Proportion cumulative de variance expliquée\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Questions :__ \n",
    "* Quelle est la proportion de variance expliquée par les deux premières composantes ? \n",
    "* Combien de composantes faudrait-il utiliser pour expliquer 80% de la variance des données ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Réponse :__\n",
    "* visuellement, 50% de la variance est expliquée par les 2 premières PC. On peut vérifier avec \n",
    "```python \n",
    "np.sum(pca.explained_variance_ratio_[:2])\n",
    "```\n",
    "ou\n",
    "```python\n",
    "np.cumsum(pca.explained_variance_ratio_)[1]\n",
    "```\n",
    "* 5 composantes (soit la moitié du nombre total de variables) permettent d'expliquer un peu plus que 80% de la variance des données. On peut vérifier avec\n",
    "```python \n",
    "np.sum(pca.explained_variance_ratio_[:5])\n",
    "```\n",
    "ou\n",
    "```python\n",
    "np.cumsum(pca.explained_variance_ratio_)[4]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Projection des données sur les deux premières composantes principales\n",
    "\n",
    "Nous allons maintenant utiliser uniquement les deux premières composantes principales.\n",
    "\n",
    "Commençons par calculer la nouvelle représentation des données, c'est-à-dire leur projection sur ces deux PC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = decomposition.PCA(n_components=2)\n",
    "pca.fit(X_scaled)\n",
    "X_projected = pca.transform(X_scaled)\n",
    "print(X_projected.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question :__ Affichez un nuage de points représentant les données selon ces deux PC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Réponse :__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "plt.scatter(X_projected[:, 0], X_projected[:, 1])\n",
    "\n",
    "plt.xlabel(\"PC 1\")\n",
    "plt.ylabel(\"PC 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question :__ Colorez chaque point du nuage de points ci-dessus en fonction du classement de l'athlète qu'il représente. Qu'en conclure sur l'interprétation de la PC1 ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Réponse :__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "plt.scatter(X_projected[:, 0], X_projected[:, 1], c=my_data['Rank'])\n",
    "\n",
    "plt.xlabel(\"PC 1\")\n",
    "plt.ylabel(\"PC 2\")\n",
    "plt.colorbar(label='classement')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les meilleurs athlètes sont plutôt à droite (PC1 > 0) et les moins bons à gauche (PC1 < 0). La première composante principale traduit la performance globale des athlètes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Interprétation des deux composantes principales\n",
    "Chaque composante principale est une combinaison linéaire des variables décrivant les données. Les poids de cette combinaison linéaire sont accesibles dans `pca.components_`.\n",
    "\n",
    "Nous pouvons visualiser non pas les individus, mais les 10 variables dans l'espace des 2 composantes principales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs = pca.components_\n",
    "print(pcs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 6))\n",
    "\n",
    "plt.scatter(pcs[0], pcs[1])\n",
    "for (x_coordinate, y_coordinate, feature_name) in zip(pcs[0], pcs[1], my_data.columns[:10]):\n",
    "    plt.text(x_coordinate, y_coordinate, feature_name)                          \n",
    "    \n",
    "plt.xlabel(\"Contribution à la PC1\")\n",
    "plt.ylabel(\"Contribution à la PC2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question :__ Quelles variables ont des contributions très similaires aux deux composantes principales ? Qu'en déduire sur leur similarité ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Réponse :__ Les performances aux épreuves de 110m haie et de 100m, ou de lancer du poids et lancer du disque, sont très similaires. On imagine volontiers qu'elles requièrent les mêmes capacités physiques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question :__ Comment interpréter le signe des contributions des variables à la première composantes principales ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Réponse :__  Les variables contribuant négativement à la PC1 correspondent aux épreuves où un bon score correspond à une petite valeur (on mesure un temps, il faut être rapide) tandis que celles contribuant positivement correspondent aux épreuves où un bon score correspond à une valeur élevée (on mesure un temps, il faut aller loin).\n",
    "\n",
    "La première PC traduit ainsi la performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question facultative :__ Comment les valeurs de `pca.explained_variance_ratio_` sont-elles obtenues ? Refaites le calcul vous-mêmes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Réponse possible\n",
    "tot_var = np.var(X_scaled, axis=0).sum()\n",
    "print((1 / tot_var) * np.var(X_projected, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 [Pour aller plus loin] Implémentation de l'ACP\n",
    "Cette partie s'inspire d'un notebook proposé par Joseph Boyd et Benoît Playe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Décomposition spectrale de la matrice de covariance\n",
    "\n",
    "Les deux premières composantes principales de X sont les deux vecteurs propres correspondant aux deux plus grandes valeurs propres de la matrice de covariance $\\Sigma = \\frac1n X^\\top X.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question :__ Utilisez `numpy.linalg` pour calculer les deux premières composantes principales de X. N'oubliez pas de travailler sur `X_scaled`. Comparez les à celles obtenues dans `pca.components_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg\n",
    "\n",
    "# Calcul de la matrice de covariance\n",
    "N = X_scaled.shape[0]\n",
    "X_cov = X_scaled.T.dot(X_scaled) / N\n",
    "\n",
    "# Décomposition spectrale\n",
    "eigenvals, eigenvecs = linalg.eig(X_cov)\n",
    "my_pcs = eigenvecs[:,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 6))\n",
    "\n",
    "plt.scatter(my_pcs[:, 0], my_pcs[:, 1])\n",
    "for (x_coordinate, y_coordinate, feature_name) in zip(my_pcs[:, 0], my_pcs[:, 1], my_data.columns[:10]):\n",
    "    plt.text(x_coordinate, y_coordinate, feature_name)                          \n",
    "    \n",
    "plt.xlabel(\"Contribution à la PC1\")\n",
    "plt.ylabel(\"Contribution à la PC2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On obtient ici les vecteurs opposés à ceux données par `sklearn` (ce qui n'est pas anormal). On peut le vérifier numériquement :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question :__ Calculez la nouvelle représentation en deux dimensions des données. Comparez à celle obtenue avec `pca.transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.abs(my_pcs.T + pcs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_new_X = X_scaled.dot(my_pcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "plt.scatter(my_new_X[:, 0], my_new_X[:, 1], c=my_data['Rank'])\n",
    "                 #cmap=plt.get_cmap('viridis'))\n",
    "\n",
    "#rank_first_indices = np.flatnonzero(my_data['Rank']==1)\n",
    "#plt.scatter(X_projected[rank_first_indices, 0], X_projected[rank_first_indices, 1], label='Gagnants')\n",
    "\n",
    "plt.xlabel(\"PC 1\")\n",
    "plt.ylabel(\"PC 2\")\n",
    "plt.colorbar(label='classement')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les axes sont inversés, comme on s'y attendait."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Décomposition en valeurs singulières"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question :__ Utiliser `linalg.svd` pour retrouver les deux premières composantes principales. Comparer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, V = linalg.svd(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_pcs = V[:2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.abs(pcs - svd_pcs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous retrouvons les mêmes PCs qu'avec `sklearn`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
